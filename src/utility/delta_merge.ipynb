{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4185b299-78a5-47ad-bae4-59864f0053c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b64c9ce-60eb-4434-bbf7-8a8393b5295d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def check_delta_table(table_name):\n",
    "    return spark.catalog.tableExists(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "188a50c9-9281-49af-b9b1-23bbf5a366f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def delta_merge(\n",
    "    source_table_name: DataFrame\n",
    "    , target_table_name: str\n",
    "    , primary_key: str\n",
    "):\n",
    "    # people_table.alias(\"target\").merge(\n",
    "    #     new_df.alias(\"source\"), \"target.id = source.id\"\n",
    "    # ).whenNotMatchedInsert(\n",
    "    #     condition='source._op = \"INSERT\"',\n",
    "    #     values={\"id\": \"source.id\", \"name\": \"source.name\", \"age\": \"source.age\"},\n",
    "    # ).whenMatchedUpdate(\n",
    "    #     condition='source._op = \"UPDATE\"',\n",
    "    #     set={\"id\": \"source.id\", \"name\": \"source.name\", \"age\": \"source.age\"},\n",
    "    # ).whenMatchedDelete(\n",
    "    #     condition='source._op = \"DELETE\"'\n",
    "    # ).execute()\n",
    "\n",
    "    target_dt = DeltaTable.forName(spark, target_table_name)\n",
    "    cols_map = {col : f\"source.{col}\" for col in source_table_name.columns}\n",
    "    merge_condition = f\"target.{primary_key} = source.{primary_key}\"\n",
    "\n",
    "    target_dt.alias(\"target\").merge(\n",
    "        source_table_name.alias(\"source\")\n",
    "        , merge_condition\n",
    "    ).whenNotMatchedInsert(\n",
    "        values=cols_map\n",
    "    ).whenMatchedUpdate(\n",
    "        set=cols_map\n",
    "    ).execute()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "delta_merge",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
